{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"make_data.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyNhIQ6QUGnnm/G325t63q/G"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"F4721Bzxzmgh","executionInfo":{"status":"ok","timestamp":1603412736473,"user_tz":-480,"elapsed":1124,"user":{"displayName":"如子","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi3ItGjzEGzUOlXTUHjOgeuVA5TICdNcY-Q1TGicA=s64","userId":"01997730851420384589"}},"outputId":"0c8c40b4-0a42-4e79-ed5c-ccc1356cb40c","colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["from google.colab import drive\n","drive.mount('/content/gdrive')\n","import os\n","os.chdir('/content/gdrive/My Drive/finch/tensorflow2/text_matching/ant/data')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"hJOhWoIHzzqV","executionInfo":{"status":"ok","timestamp":1603412736478,"user_tz":-480,"elapsed":1056,"user":{"displayName":"如子","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi3ItGjzEGzUOlXTUHjOgeuVA5TICdNcY-Q1TGicA=s64","userId":"01997730851420384589"}}},"source":["from collections import Counter\n","from pathlib import Path\n","\n","import json\n","import numpy as np"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"mHyiubX906j_","executionInfo":{"status":"ok","timestamp":1603412740602,"user_tz":-480,"elapsed":1406,"user":{"displayName":"如子","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi3ItGjzEGzUOlXTUHjOgeuVA5TICdNcY-Q1TGicA=s64","userId":"01997730851420384589"}},"outputId":"db4a2505-0bc6-425e-a411-f208a5bba34c","colab":{"base_uri":"https://localhost:8080/","height":69}},"source":["lens_1, lens_2 = [], []\n","counter = Counter()\n","\n","with open('./train.json') as f:\n","  for line in f:\n","    line = json.loads(line.rstrip())\n","    text1, text2, label = line['sentence1'], line['sentence2'], line['label']\n","    counter.update(list(text1))\n","    counter.update(list(text2))\n","    lens_1.append(len(list(text1)))\n","    lens_2.append(len(list(text2)))\n","\n","print('Average Length 1:', sum(lens_1) / len(lens_1))\n","print('Average Length 2:', sum(lens_2) / len(lens_2))\n","chars = [w for w, freq in counter.most_common()]\n","\n","Path('../vocab').mkdir(exist_ok=True)\n","\n","with open('../vocab/char.txt', 'w') as f:\n","  f.write('<pad>'+'\\n')\n","  for c in chars:\n","    f.write(c+'\\n')\n","\n","print('Chars:', len(chars))"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Average Length 1: 13.33287703151395\n","Average Length 2: 13.399720393778761\n","Chars: 1704\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"NazIbf_L194r","executionInfo":{"status":"ok","timestamp":1603412804492,"user_tz":-480,"elapsed":63777,"user":{"displayName":"如子","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi3ItGjzEGzUOlXTUHjOgeuVA5TICdNcY-Q1TGicA=s64","userId":"01997730851420384589"}},"outputId":"c4a4040a-899d-4c9f-bea9-efce0799b4d7","colab":{"base_uri":"https://localhost:8080/","height":399}},"source":["char2idx = {}\n","with open('../vocab/char.txt') as f:\n","  for i, line in enumerate(f):\n","    line = line.rstrip('\\n')\n","    char2idx[line] = i\n","\n","embedding = np.zeros((len(char2idx)+1, 300)) # + 1 for unknown word\n","\n","with open('./cc.zh.300.vec') as f:\n","  count = 0\n","  for i, line in enumerate(f):\n","    if i == 0:\n","      continue\n","    if i % 100000 == 0:\n","      print('- At line {}'.format(i))\n","    line = line.rstrip()\n","    sp = line.split(' ')\n","    word, vec = sp[0], sp[1:]\n","    if word in char2idx:\n","      count += 1\n","      embedding[char2idx[word]] = np.asarray(vec, dtype='float32')\n","      \n","print(\"[%d / %d] characters have found pre-trained values\"%(count, len(char2idx)))\n","np.save('../vocab/char.npy', embedding)\n","print('Saved ../vocab/char.npy')"],"execution_count":4,"outputs":[{"output_type":"stream","text":["- At line 100000\n","- At line 200000\n","- At line 300000\n","- At line 400000\n","- At line 500000\n","- At line 600000\n","- At line 700000\n","- At line 800000\n","- At line 900000\n","- At line 1000000\n","- At line 1100000\n","- At line 1200000\n","- At line 1300000\n","- At line 1400000\n","- At line 1500000\n","- At line 1600000\n","- At line 1700000\n","- At line 1800000\n","- At line 1900000\n","- At line 2000000\n","[1693 / 1705] characters have found pre-trained values\n","Saved ../vocab/char.npy\n"],"name":"stdout"}]}]}